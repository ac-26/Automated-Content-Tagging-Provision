{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbemzX6jX3rlIvGW1wy2Vx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ac-26/Automated-Content-Tagging-Provision/blob/main/content_tagging_v6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s3Zoho773RWc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import Counter\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder:\n",
        "  #initialization function\n",
        "  def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      self.model = AutoModel.from_pretrained(model_name)\n",
        "      self.model.eval()\n",
        "\n",
        "  #encodes text\n",
        "  def encode_text(self, text: str) -> np.ndarray:\n",
        "    inputs = self.tokenizer(text, return_tensors=\"pt\",truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "      outputs = self.model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings.numpy().flatten()"
      ],
      "metadata": {
        "id": "3XdI1bkm3c7_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **This was my initial approach when I was brainstorming to build this system, this is not being used currently in our system, IT IS ONLY KEPT FOR DEMONSTRATION OF PROJECT EVOLUTION.**"
      ],
      "metadata": {
        "id": "DoUBN2OZZb1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class TagVocabulary:\n",
        "#   #initialization function\n",
        "#   def __init__(self):\n",
        "#     self.tags = [\n",
        "#             # Content Creation\n",
        "#             \"Content Writing\", \"Copywriting\", \"Blog Writing\", \"Article Writing\",\n",
        "#             \"Creative Writing\", \"Technical Writing\", \"Content Strategy\",\n",
        "\n",
        "#             # Marketing\n",
        "#             \"Social Media Marketing\", \"Digital Marketing\", \"Email Marketing\",\n",
        "#             \"Marketing Strategy\", \"Brand Marketing\", \"Influencer Marketing\",\n",
        "\n",
        "#             # Social Media\n",
        "#             \"Social Media\", \"Facebook Marketing\", \"Instagram Marketing\",\n",
        "#             \"Twitter Marketing\", \"LinkedIn Marketing\", \"TikTok Marketing\",\n",
        "\n",
        "#             # Analytics & Testing\n",
        "#             \"A/B Testing\", \"Analytics\", \"Performance Tracking\", \"Data Analysis\",\n",
        "#             \"Audience Research\", \"Market Research\",\n",
        "\n",
        "#             # Advertising\n",
        "#             \"Online Advertising\", \"Social Media Ads\", \"Google Ads\",\n",
        "#             \"Facebook Ads\", \"Digital Advertising\",\n",
        "\n",
        "#             # Skills & Techniques\n",
        "#             \"Communication Skills\", \"Writing Skills\", \"Creative Skills\",\n",
        "#             \"Marketing Skills\", \"Design Skills\",\n",
        "\n",
        "#             # Strategy & Planning\n",
        "#             \"Content Planning\", \"Marketing Planning\", \"Campaign Strategy\",\n",
        "#             \"Audience Targeting\", \"Customer Engagement\"\n",
        "#         ]\n",
        "\n",
        "#     print(f\"Tag vocabulary initialized with {len(self.tags)} tags\")\n",
        "\n",
        "#     #this will return list of tags in our vocabulary\n",
        "#     def get_tags(self) -> List[str]:\n",
        "#         return self.tags.copy()\n",
        "\n",
        "#     #this will add a new tag in our vocabulary\n",
        "#     def add_tag(self, new_tag: str):\n",
        "#         if new_tag not in self.tags:\n",
        "#             self.tags.append(new_tag)\n",
        "#             print(f\"Added new tag: {new_tag}\")\n",
        "#         else:\n",
        "#             print(f\"Tag '{new_tag}' already exists\")"
      ],
      "metadata": {
        "id": "Ulg6UPSU6Pso"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class BasicTagger:\n",
        "#   #initializer function\n",
        "#     def __init__(self):\n",
        "#       self.encoder = TextEncoder()\n",
        "#       self.vocabulary = TagVocabulary()\n",
        "\n",
        "#       self.tag_embeddings = self._encode_all_tags()\n",
        "\n",
        "#     #function to encode all tags before hand\n",
        "#     def _encode_all_tags(self) -> Dict[str, np.ndarray]:\n",
        "#         tag_embeddings = {}\n",
        "#         for tag in self.vocabulary.tags:\n",
        "#             embedding = self.encoder.encode_text(tag)\n",
        "#             tag_embeddings[tag] = embedding\n",
        "\n",
        "#         return tag_embeddings\n",
        "\n",
        "#     #finds tags from our vocaublary that are applicable according to our text input\n",
        "#     def find_matching_tags(self, input_text: str, top_k: int = 10) -> List[Tuple[str, float]]:\n",
        "#         # Encode the input text\n",
        "#         input_embedding = self.encoder.encode_text(input_text)\n",
        "\n",
        "#         similarities = []\n",
        "\n",
        "#         for tag_name, tag_embedding in self.tag_embeddings.items():\n",
        "#             # Calculate cosine similarity\n",
        "#             similarity = cosine_similarity(\n",
        "#                 input_embedding.reshape(1, -1),\n",
        "#                 tag_embedding.reshape(1, -1)\n",
        "#             )[0][0]\n",
        "\n",
        "#             similarities.append((tag_name, float(similarity)))\n",
        "\n",
        "#         similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "#         return similarities[:top_k]"
      ],
      "metadata": {
        "id": "vcARCTEQ6ZOb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Test our basic tagger\n",
        "# def test_basic_tagger():\n",
        "#     tagger = BasicTagger()\n",
        "\n",
        "#     test_text = \"\"\"\n",
        "#     Creating social media posts is a great way to hone your content writing skills.\n",
        "#     Since posts are typically very short, snappy, and quick, you can easily try out\n",
        "#     different styles of writing and see what people respond to. It's easy to change\n",
        "#     direction and adapt if you need to tweak your writing style since social media\n",
        "#     posts are typically fluid and changeable by nature. You can also practice A/B\n",
        "#     testing with your social media adsâ€”try writing two different posts and sending\n",
        "#     it to similar demographics and see which one performs better.\n",
        "#     \"\"\"\n",
        "\n",
        "#     print(\"Input text:\")\n",
        "#     print(test_text)\n",
        "\n",
        "#     # Find matching tags\n",
        "#     matching_tags = tagger.find_matching_tags(test_text, top_k=15)\n",
        "\n",
        "#     print(\"Top matching tags:\")\n",
        "#     for i, (tag, score) in enumerate(matching_tags, 1):\n",
        "#         print(f\"{i:2d}. {tag:<25} (Score: {score:.3f})\")"
      ],
      "metadata": {
        "id": "5O6vyybt_o1o"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#   test_basic_tagger()"
      ],
      "metadata": {
        "id": "r2BVRTDjbJGX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dynamic Tag Generation**"
      ],
      "metadata": {
        "id": "KlTi8gxagMGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **This is the approach that is being followed right now, it is able to fix the problems and limitation that we were facing in the above approach**"
      ],
      "metadata": {
        "id": "qzTuHgYDcklF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this extracts key phrases from text using linguistic patterns and statistical methods.\n",
        "class KeyPhraseExtractor:\n",
        "    def __init__(self):\n",
        "        # trying to use spacy model for linguistic analysis\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except:\n",
        "            print(\"Please install spacy model: python -m spacy download en_core_web_sm\")\n",
        "            raise\n",
        "\n",
        "        #I have used Parts of Speech(POS) concept for tag extraction\n",
        "        self.phrase_patterns = [\n",
        "            #1-word phrases\n",
        "            [\"NOUN\"],\n",
        "            [\"PROPN\"],\n",
        "\n",
        "            # 2-word phrases\n",
        "            [\"ADJ\", \"NOUN\"],\n",
        "            [\"ADJ\", \"PROPN\"],\n",
        "            [\"NOUN\", \"NOUN\"],\n",
        "            [\"PROPN\", \"NOUN\"],\n",
        "            [\"NOUN\", \"PROPN\"],\n",
        "\n",
        "            # 3-word phrases\n",
        "            [\"ADJ\", \"NOUN\", \"NOUN\"],\n",
        "            [\"NOUN\", \"NOUN\", \"NOUN\"],\n",
        "            [\"PROPN\", \"PROPN\", \"PROPN\"],\n",
        "            [\"ADJ\", \"ADJ\", \"NOUN\"],\n",
        "            [\"NOUN\", \"VERB\", \"NOUN\"],\n",
        "\n",
        "            # 4-word phrases\n",
        "            [\"ADJ\", \"NOUN\", \"NOUN\", \"NOUN\"],\n",
        "            [\"NOUN\", \"NOUN\", \"NOUN\", \"NOUN\"],\n",
        "            [\"ADJ\", \"ADJ\", \"NOUN\", \"NOUN\"],\n",
        "            # [\"NOUN\", \"NOUN\", \"VERB\", \"NOUN\"],\n",
        "            # [\"ADJ\", \"NOUN\", \"VERB\", \"NOUN\"],\n",
        "            # [\"NOUN\", \"NOUN\", \"NOUN\", \"VERB\"],\n",
        "\n",
        "            # Verb forms (gerunds)\n",
        "            # [\"VERB\"],  # Will filter for -ing forms\n",
        "            # [\"ADJ\", \"VERB\"],\n",
        "        ]\n",
        "\n",
        "        #common compound terms that should stay together\n",
        "        self.compound_terms = {\n",
        "            \"machine learning\", \"deep learning\", \"natural language processing\",\n",
        "            \"neural network\", \"data science\", \"artificial intelligence\",\n",
        "            \"computer vision\", \"big data\", \"decision making\",\n",
        "            \"supply chain\", \"customer relationship\", \"human resources\",\n",
        "            \"business process\"\n",
        "        }\n",
        "\n",
        "\n",
        "    #applying Named Entity Recognition(NER) for extracting entities like ORG, PERSON, GPE, DATE, etc.\n",
        "    def extract_named_entities(self, text: str) -> List[Tuple[str, str, int]]:\n",
        "      #finalised using spacy NER, i tried BERT based NER as well but it was also giving near to similar results\n",
        "      #hence why to waste time and space and make our model more complex to understand\n",
        "      doc = self.nlp(text)\n",
        "\n",
        "      #count frequency of each entity\n",
        "      entity_counter = Counter()\n",
        "      entity_labels = {}\n",
        "\n",
        "\n",
        "      for ent in doc.ents:\n",
        "          entity_norm = ent.text.lower().strip()\n",
        "          if ent.label_ == \"CARDINAL\":\n",
        "              continue\n",
        "\n",
        "          #dont use very short entities. (i did this because after some testing on phrases NER was catching some garbage as well)\n",
        "          if len(ent.text) < 3 and ent.label_ not in [\"ORG\", \"GPE\", \"PERSON\"]:\n",
        "              continue\n",
        "\n",
        "          entity_counter[entity_norm] += 1\n",
        "          #storing original label\n",
        "          if entity_norm not in entity_labels:\n",
        "              entity_labels[entity_norm] = (ent.text, ent.label_)\n",
        "\n",
        "      #putting the output in proper format\n",
        "      entities = []\n",
        "      for norm_text, freq in entity_counter.items():\n",
        "          original_text, label = entity_labels[norm_text]\n",
        "          entities.append((original_text, label, freq))\n",
        "\n",
        "      return entities\n",
        "\n",
        "\n",
        "    #using noun_chunks from spacy\n",
        "    def extract_noun_chunks(self, text: str) -> List[Tuple[str, int]]:\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        chunk_counter = Counter()\n",
        "\n",
        "        # Words that shouldn't start/end chunks\n",
        "        bad_starts = {'which', 'that', 'this', 'these', 'those', 'your', 'my', 'our', 'their', 'a', 'an', 'the', \"it's\", 'its'}\n",
        "        bad_ends = {'one', 'ones', 'thing', 'things', 'way', 'ways'}\n",
        "\n",
        "        for chunk in doc.noun_chunks:\n",
        "            chunk_text = chunk.text.strip()\n",
        "            chunk_words = chunk_text.lower().split()\n",
        "\n",
        "            # Skip if starts/ends badly\n",
        "            if chunk_words and (chunk_words[0] in bad_starts or chunk_words[-1] in bad_ends):\n",
        "                continue\n",
        "\n",
        "            #taking only multiword phrases because one word phrase will already get caught in POS\n",
        "            if len(chunk_words) >= 2:\n",
        "                chunk_lower = chunk_text.lower()\n",
        "                chunk_counter[chunk_lower] += 1\n",
        "\n",
        "        return list(chunk_counter.items())\n",
        "\n",
        "\n",
        "    #updating extract phrases function to combine POS patterns from before and noun chunks and NER entitites as well\n",
        "    def extract_phrases_with_metadata(self, text: str, min_freq: int = 1) -> List[Tuple[str, int, dict]]:\n",
        "      doc = self.nlp(text.lower())\n",
        "\n",
        "      phrase_data = {} #this will contain freqency, source, entity type and original twxt\n",
        "\n",
        "      #extraction from ner\n",
        "      ner_entities = self.extract_named_entities(text)\n",
        "      for entity_text, entity_label, freq in ner_entities:\n",
        "          phrase_lower = entity_text.lower()\n",
        "\n",
        "          if phrase_lower not in phrase_data:\n",
        "              phrase_data[phrase_lower] = {\n",
        "                  'freq': freq,\n",
        "                  'source': 'ner',\n",
        "                  'entity_type': entity_label,\n",
        "                  'original_text': entity_text\n",
        "              }\n",
        "          else:\n",
        "              if freq > phrase_data[phrase_lower]['freq']:\n",
        "                  phrase_data[phrase_lower]['freq'] = freq\n",
        "                  phrase_data[phrase_lower]['entity_type'] = entity_label\n",
        "\n",
        "      #extraction fron noun_chunks\n",
        "      noun_chunks = self.extract_noun_chunks(text)\n",
        "      for chunk_text, freq in noun_chunks:\n",
        "          if chunk_text not in phrase_data:\n",
        "              phrase_data[chunk_text] = {\n",
        "                  'freq': freq,\n",
        "                  'source': 'noun_chunk',\n",
        "                  'entity_type': None,\n",
        "                  'original_text': chunk_text\n",
        "              }\n",
        "\n",
        "      #small cbeck in compund terms(trial)\n",
        "      text_lower = text.lower()\n",
        "      for compound in self.compound_terms:\n",
        "          count = text_lower.count(compound)\n",
        "          if count > 0:\n",
        "              if compound not in phrase_data:\n",
        "                  phrase_data[compound] = {\n",
        "                      'freq': count,\n",
        "                      'source': 'pos',\n",
        "                      'entity_type': None,\n",
        "                      'original_text': compound\n",
        "                  }\n",
        "              else:\n",
        "                  phrase_data[compound]['freq'] = max(phrase_data[compound]['freq'], count)\n",
        "\n",
        "      #POS matching algorithm\n",
        "      for sentence in doc.sents:\n",
        "          tokens = list(sentence)\n",
        "          for start_idx in range(len(tokens)):\n",
        "              for pattern in self.phrase_patterns:\n",
        "                  end_idx = start_idx + len(pattern)\n",
        "                  if end_idx <= len(tokens):\n",
        "                      span_tokens = tokens[start_idx:end_idx]\n",
        "                      pos_sequence = [token.pos_ for token in span_tokens]\n",
        "\n",
        "                      if pos_sequence == pattern:\n",
        "                          phrase_tokens = []\n",
        "                          valid = True\n",
        "\n",
        "                          for i, token in enumerate(span_tokens):\n",
        "                              if len(pattern) == 1:\n",
        "                                  if token.is_stop:\n",
        "                                      valid = False\n",
        "                                      break\n",
        "\n",
        "                                  meaningless_words = {\n",
        "                                      'people', 'person', 'thing', 'things', 'way', 'ways',\n",
        "                                      'time', 'times', 'place', 'places', 'nature', 'direction',\n",
        "                                      'fact', 'case', 'point', 'example', 'kind', 'type',\n",
        "                                      'one', 'two', 'three', 'four', 'five'\n",
        "                                  }\n",
        "\n",
        "                                  if token.text.lower() in meaningless_words:\n",
        "                                      valid = False\n",
        "                                      break\n",
        "\n",
        "                                  if token.pos_ == \"VERB\":\n",
        "                                      if not token.text.endswith(\"ing\"):\n",
        "                                          valid = False\n",
        "                                          break\n",
        "\n",
        "                                  if len(token.text) < 3:\n",
        "                                      valid = False\n",
        "                                      break\n",
        "\n",
        "                              phrase_tokens.append(token.text)\n",
        "\n",
        "                          if valid and phrase_tokens:\n",
        "                              phrase = \" \".join(phrase_tokens)\n",
        "                              phrase = re.sub(r'\\s+', ' ', phrase).strip()\n",
        "\n",
        "                              is_subset = False\n",
        "                              for existing in phrase_data:\n",
        "                                  if phrase in existing and phrase != existing:\n",
        "                                      is_subset = True\n",
        "                                      break\n",
        "\n",
        "                              if not is_subset and phrase:\n",
        "                                  if phrase not in phrase_data:\n",
        "                                      phrase_data[phrase] = {\n",
        "                                          'freq': 1,\n",
        "                                          'source': 'pos',\n",
        "                                          'entity_type': None,\n",
        "                                          'original_text': phrase\n",
        "                                      }\n",
        "                                  else:\n",
        "                                      phrase_data[phrase]['freq'] += 1\n",
        "      #clesning for output\n",
        "      phrases_with_metadata = []\n",
        "      for phrase, data in phrase_data.items():\n",
        "          if data['freq'] >= min_freq:\n",
        "              metadata = {\n",
        "                  'source': data['source'],\n",
        "                  'entity_type': data['entity_type']\n",
        "              }\n",
        "              display_text = data.get('original_text', phrase)\n",
        "              phrases_with_metadata.append((display_text, data['freq'], metadata))\n",
        "\n",
        "      phrases_with_metadata.sort(key=lambda x: (x[1], len(x[0].split())), reverse=True)\n",
        "\n",
        "      return phrases_with_metadata\n",
        "\n",
        "\n",
        "    def extract_phrases(self, text: str, min_freq: int = 1) -> List[Tuple[str, int]]:\n",
        "        phrases_with_metadata = self.extract_phrases_with_metadata(text, min_freq)\n",
        "        return [(phrase, freq) for phrase, freq, metadata in phrases_with_metadata]"
      ],
      "metadata": {
        "id": "3F2iiLgcAIZH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is used to score adn filter the phrases that we extracted above to identify best tags in our input sentance\n",
        "class PhraseScorer:\n",
        "    def __init__(self, encoder=None):\n",
        "\n",
        "        self.encoder = encoder\n",
        "\n",
        "        # Common/generic words that make poor tags\n",
        "        self.generic_words = {\n",
        "            'way', 'ways', 'thing', 'things', 'people', 'person', 'time', 'times',\n",
        "            'place', 'places', 'day', 'days', 'year', 'years', 'good', 'bad',\n",
        "            'great', 'nice', 'sure', 'certain', 'different', 'same', 'other',\n",
        "            'new', 'old', 'high', 'low', 'large', 'small', 'long', 'short',\n",
        "            'easy', 'hard', 'simple', 'complex', 'nature', 'type', 'types',\n",
        "            'kind', 'kinds', 'lot', 'lots', 'direction', 'need', 'needs'\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "    #encorporated NER entities to have higher weightage\n",
        "    def calculate_phrase_scores_with_ner(self, phrases_with_metadata: List[Tuple[str, int, dict]],\n",
        "                                    text_length: int) -> List[Tuple[str, float, dict]]:\n",
        "\n",
        "      scored_phrases = []\n",
        "\n",
        "      #get max frequency for normalization\n",
        "      max_freq = 1\n",
        "      if phrases_with_metadata:\n",
        "          frequencies = []\n",
        "          for phrase, freq, metadata in phrases_with_metadata:\n",
        "              frequencies.append(freq)\n",
        "          max_freq = max(frequencies)\n",
        "\n",
        "      for phrase, freq, metadata in phrases_with_metadata:\n",
        "          #initialize scores\n",
        "          scores = {\n",
        "              'frequency': 0.0,\n",
        "              'specificity': 0.0,\n",
        "              'length': 0.0,\n",
        "              'completeness': 0.0,\n",
        "              'entity_bonus': 0.0\n",
        "          }\n",
        "\n",
        "          #frequency score\n",
        "          scores['frequency'] = min(freq / max_freq, 1.0) * 0.3\n",
        "\n",
        "          #secificity score\n",
        "          words = phrase.lower().split()\n",
        "          generic_count = sum(1 for word in words if word in self.generic_words)\n",
        "          scores['specificity'] = (1 - generic_count / len(words)) * 0.25\n",
        "\n",
        "          #length score\n",
        "          if len(words) == 1:\n",
        "              scores['length'] = 0.65\n",
        "          elif len(words) == 2:\n",
        "              scores['length'] = 0.85\n",
        "          elif len(words) == 3:\n",
        "              scores['length'] = 0.90\n",
        "          elif len(words) == 4:\n",
        "              scores['length'] = 1.0\n",
        "          else:\n",
        "              scores['length'] = 0.4\n",
        "          scores['length'] *= 0.15\n",
        "\n",
        "          # completeness score\n",
        "          incomplete_markers = {'of', 'to', 'for', 'with', 'and', 'or', 'but', 'the', 'a', 'an'}\n",
        "          is_complete = (words[0] not in incomplete_markers and\n",
        "                        words[-1] not in incomplete_markers)\n",
        "          scores['completeness'] = 1.0 if is_complete else 0.5\n",
        "          scores['completeness'] *= 0.05\n",
        "\n",
        "          #-----------------------------------------------------------------\n",
        "          #NER scores\n",
        "          if metadata.get('source') == 'ner' and metadata.get('entity_type'):\n",
        "            entity_type = metadata['entity_type']\n",
        "            if entity_type == 'PERSON':\n",
        "                scores['entity_bonus'] = 0.05  # Was 0.20\n",
        "            elif entity_type == 'ORG':\n",
        "                scores['entity_bonus'] = 0.05  # Was 0.20\n",
        "            elif entity_type == 'GPE':\n",
        "                scores['entity_bonus'] = 0.05  # Was 0.25\n",
        "            elif entity_type == 'DATE':\n",
        "                scores['entity_bonus'] = 0.03  # Was 0.15\n",
        "            else:\n",
        "                scores['entity_bonus'] = 0.02  # Was 0.10\n",
        "\n",
        "          #calculate total score\n",
        "          total_score = sum(scores.values())\n",
        "\n",
        "          scored_phrases.append((phrase, total_score, metadata))\n",
        "\n",
        "      scored_phrases.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "      return scored_phrases\n",
        "\n",
        "\n",
        "\n",
        "    def semantic_clustering(self, phrases_with_scores: List[Tuple[str, float]],\n",
        "                          encoder, similarity_threshold: float = 0.85) -> List[Tuple[str, float]]:\n",
        "\n",
        "        if not phrases_with_scores:\n",
        "            return []\n",
        "\n",
        "        #extract phrases and scores\n",
        "        phrases = [phrase for phrase, _ in phrases_with_scores]\n",
        "        scores = [score for _, score in phrases_with_scores]\n",
        "\n",
        "        #generate embeddings for all phrases\n",
        "\n",
        "        embeddings = []\n",
        "        for phrase in phrases:\n",
        "            embedding = encoder.encode_text(phrase)\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        embeddings = np.array(embeddings)\n",
        "\n",
        "        #calculate pairwise similarities\n",
        "        similarities = cosine_similarity(embeddings)\n",
        "\n",
        "        #check for substring relationships and boost their similarity\n",
        "        for i in range(len(phrases)):\n",
        "          for j in range(i + 1, len(phrases)):\n",
        "              phrase_i_lower = phrases[i].lower()\n",
        "              phrase_j_lower = phrases[j].lower()\n",
        "\n",
        "              #if one phrase contains the other, boost similarity\n",
        "              if phrase_i_lower in phrase_j_lower or phrase_j_lower in phrase_i_lower:\n",
        "                  #set similarity to at least 90% for substring matches\n",
        "                  similarities[i][j] = max(similarities[i][j], 0.9)\n",
        "                  similarities[j][i] = similarities[i][j]\n",
        "\n",
        "\n",
        "                  # print(f\"DEBUG: Boosted similarity between '{phrases[i]}' and '{phrases[j]}' to {similarities[i][j]:.3f}\")\n",
        "\n",
        "\n",
        "        # Simple clustering: for each phrase, find all phrases similar to it\n",
        "        clustered = set()  # Keep track of phrases already assigned to a cluster\n",
        "        clusters = []\n",
        "\n",
        "        for i in range(len(phrases)):\n",
        "            if i in clustered:\n",
        "                continue\n",
        "\n",
        "            #start a new cluster with phrase i\n",
        "            cluster = [(i, phrases[i], scores[i])]\n",
        "            clustered.add(i)\n",
        "\n",
        "            #find all phrases similar to phrase i\n",
        "            for j in range(i + 1, len(phrases)):\n",
        "                if j in clustered:\n",
        "                    continue\n",
        "\n",
        "                if similarities[i][j] >= similarity_threshold:\n",
        "                    cluster.append((j, phrases[j], scores[j]))\n",
        "                    clustered.add(j)\n",
        "\n",
        "            clusters.append(cluster)\n",
        "\n",
        "        #select best phrase from each cluster\n",
        "        best_phrases = []\n",
        "\n",
        "        for cluster in clusters:\n",
        "            #sort by score (descending) and phrase length (descending for tie-breaking)\n",
        "            cluster.sort(key=lambda x: (x[2], len(x[1])), reverse=True)\n",
        "\n",
        "            #add the best phrase from this cluster\n",
        "            _, best_phrase, best_score = cluster[0]\n",
        "            best_phrases.append((best_phrase, best_score))\n",
        "\n",
        "            #debuging--\n",
        "            # if len(cluster) > 1:\n",
        "            #     merged = [item[1] for item in cluster[1:]]\n",
        "            #     print(f\"Merged into '{best_phrase}': {merged}\")\n",
        "\n",
        "\n",
        "        best_phrases.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return best_phrases\n",
        "\n",
        "    #This removes similar or redundant phrases keeping the ones that are most meaningful\n",
        "    def filter_similar_phrases_with_metadata(self, scored_phrases: List[Tuple[str, float, dict]],\n",
        "                                        similarity_threshold: float = 0.85) -> List[Tuple[str, float, dict]]:\n",
        "          if not scored_phrases:\n",
        "              return []\n",
        "\n",
        "          filtered = []\n",
        "\n",
        "          embedding_cache = {}\n",
        "\n",
        "          for phrase, score, metadata in scored_phrases:\n",
        "              phrase_lower = phrase.lower()\n",
        "              words = set(phrase_lower.split())\n",
        "\n",
        "              should_keep = True\n",
        "              phrases_to_remove = []\n",
        "\n",
        "              #calculate embedding for current phrase if we have encoder\n",
        "              if self.encoder and phrase_lower not in embedding_cache:\n",
        "                  embedding_cache[phrase_lower] = self.encoder.encode_text(phrase)\n",
        "\n",
        "              for i, (kept_phrase, kept_score, kept_metadata) in enumerate(filtered):\n",
        "                  kept_lower = kept_phrase.lower()\n",
        "                  kept_words = set(kept_lower.split())\n",
        "\n",
        "                  #skip if exact same phrase\n",
        "                  if phrase_lower == kept_lower:\n",
        "                      should_keep = False\n",
        "                      break\n",
        "\n",
        "                  #check semantic similarity if encoder available\n",
        "                  if self.encoder:\n",
        "                      #get or calculate embedding for kept phrase\n",
        "                      if kept_lower not in embedding_cache:\n",
        "                          embedding_cache[kept_lower] = self.encoder.encode_text(kept_phrase)\n",
        "\n",
        "                      #calculate similarity\n",
        "                      semantic_similarity = cosine_similarity(\n",
        "                          embedding_cache[phrase_lower].reshape(1, -1),\n",
        "                          embedding_cache[kept_lower].reshape(1, -1)\n",
        "                      )[0][0]\n",
        "\n",
        "                      if semantic_similarity > similarity_threshold:\n",
        "                          #prefer NER entities\n",
        "                          current_is_ner = metadata.get('source') == 'ner'\n",
        "                          kept_is_ner = kept_metadata.get('source') == 'ner'\n",
        "\n",
        "                          if current_is_ner and not kept_is_ner:\n",
        "                              phrases_to_remove.append(i)\n",
        "                          elif kept_is_ner and not current_is_ner:\n",
        "                              should_keep = False\n",
        "                              break\n",
        "                          else:\n",
        "                              #same source type - prefer shorter/cleaner or higher score\n",
        "                              if len(words) < len(kept_words) and score > kept_score * 0.9:\n",
        "                                  phrases_to_remove.append(i)\n",
        "                              elif score > kept_score:\n",
        "                                  phrases_to_remove.append(i)\n",
        "                              else:\n",
        "                                  should_keep = False\n",
        "                                  break\n",
        "\n",
        "                  #original subset logic\n",
        "                  elif words.issubset(kept_words) or kept_words.issubset(words):\n",
        "                      len_diff = abs(len(words) - len(kept_words))\n",
        "                      score_diff = abs(score - kept_score)\n",
        "\n",
        "                      if score_diff < 0.15 and len_diff >= 1:\n",
        "                          current_is_ner = metadata.get('source') == 'ner'\n",
        "                          kept_is_ner = kept_metadata.get('source') == 'ner'\n",
        "\n",
        "                          if current_is_ner and not kept_is_ner:\n",
        "                              phrases_to_remove.append(i)\n",
        "                          elif kept_is_ner and not current_is_ner:\n",
        "                              should_keep = False\n",
        "                              break\n",
        "                          else:\n",
        "                              if len(words) > len(kept_words):\n",
        "                                  phrases_to_remove.append(i)\n",
        "                              else:\n",
        "                                  should_keep = False\n",
        "                                  break\n",
        "                      else:\n",
        "                          if score > kept_score:\n",
        "                              phrases_to_remove.append(i)\n",
        "                          else:\n",
        "                              should_keep = False\n",
        "                              break\n",
        "\n",
        "              if phrases_to_remove:\n",
        "                  for idx in reversed(phrases_to_remove):\n",
        "                      filtered.pop(idx)\n",
        "\n",
        "              if should_keep:\n",
        "                  filtered.append((phrase, score, metadata))\n",
        "\n",
        "          return filtered\n",
        "\n",
        "    #calculate score without ner\n",
        "    def calculate_phrase_scores(self, phrases: List[Tuple[str, int]],\n",
        "                                text_length: int) -> List[Tuple[str, float]]:\n",
        "        scored_phrases = []\n",
        "\n",
        "        #get max frequency for normalization\n",
        "        max_freq = max([freq for _, freq in phrases]) if phrases else 1\n",
        "\n",
        "        for phrase, freq in phrases:\n",
        "            #initialize scores\n",
        "            scores = {\n",
        "                'frequency': 0.0,\n",
        "                'specificity': 0.0,\n",
        "                'length': 0.0,\n",
        "                'completeness': 0.0\n",
        "            }\n",
        "\n",
        "            #frequency score\n",
        "            scores['frequency'] = min(freq / max_freq, 1.0) * 0.3\n",
        "\n",
        "            #specificity score\n",
        "            words = phrase.lower().split()\n",
        "            generic_count = sum(1 for word in words if word in self.generic_words)\n",
        "            scores['specificity'] = (1 - generic_count / len(words)) * 0.25\n",
        "\n",
        "            # 3.length score\n",
        "            if len(words) == 1:\n",
        "                scores['length'] = 0.65\n",
        "            elif len(words) == 2:\n",
        "                scores['length'] = 0.85\n",
        "            elif len(words) == 3:\n",
        "                scores['length'] = 0.90\n",
        "            elif len(words) == 4:\n",
        "                scores['length'] = 1.0\n",
        "            else:\n",
        "                scores['length'] = 0.4\n",
        "            scores['length'] *= 0.15\n",
        "\n",
        "            #trial\n",
        "            incomplete_markers = {'of', 'to', 'for', 'with', 'and', 'or', 'but', 'the', 'a', 'an'}\n",
        "            is_complete = (words[0] not in incomplete_markers and\n",
        "                          words[-1] not in incomplete_markers)\n",
        "            scores['completeness'] = 1.0 if is_complete else 0.5\n",
        "            scores['completeness'] *= 0.1\n",
        "\n",
        "            #calculate score\n",
        "            total_score = sum(scores.values())\n",
        "\n",
        "            scored_phrases.append((phrase, total_score))\n",
        "\n",
        "        scored_phrases.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return scored_phrases\n",
        "\n",
        "\n",
        "    def filter_similar_phrases(self, scored_phrases: List[Tuple[str, float]]) -> List[Tuple[str, float]]:\n",
        "        if not scored_phrases:\n",
        "            return []\n",
        "\n",
        "        filtered = []\n",
        "\n",
        "        for phrase, score in scored_phrases:\n",
        "            phrase_lower = phrase.lower()\n",
        "            words = set(phrase_lower.split())\n",
        "\n",
        "            should_keep = True\n",
        "            phrases_to_remove = []\n",
        "\n",
        "            for i, (kept_phrase, kept_score) in enumerate(filtered):\n",
        "                kept_lower = kept_phrase.lower()\n",
        "                kept_words = set(kept_lower.split())\n",
        "\n",
        "                #skip if exact same phrase is there\n",
        "                if phrase_lower == kept_lower:\n",
        "                    should_keep = False\n",
        "                    break\n",
        "\n",
        "                #handle subset relationships\n",
        "                if words.issubset(kept_words) or kept_words.issubset(words):\n",
        "                    len_diff = abs(len(words) - len(kept_words))\n",
        "                    score_diff = abs(score - kept_score)\n",
        "\n",
        "                    if score_diff < 0.15 and len_diff >= 1:\n",
        "                        if len(words) > len(kept_words):\n",
        "                            phrases_to_remove.append(i)\n",
        "                        else:\n",
        "                            should_keep = False\n",
        "                            break\n",
        "                    else:\n",
        "                        if score > kept_score:\n",
        "                            phrases_to_remove.append(i)\n",
        "                        else:\n",
        "                            should_keep = False\n",
        "                            break\n",
        "\n",
        "            if phrases_to_remove:\n",
        "                for idx in reversed(phrases_to_remove):\n",
        "                    filtered.pop(idx)\n",
        "\n",
        "            if should_keep:\n",
        "                filtered.append((phrase, score))\n",
        "\n",
        "        return filtered"
      ],
      "metadata": {
        "id": "037iMS-hi-61"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete dynamic tagging class\n",
        "class DynamicTagger:\n",
        "    def __init__(self, encoder_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        self.encoder = TextEncoder(encoder_model)\n",
        "        self.extractor = KeyPhraseExtractor()\n",
        "        self.scorer = PhraseScorer(self.encoder)\n",
        "        print(\"DynamicTagger ready!\", flush=True)\n",
        "\n",
        "    def generate_tags(self, text: str, max_tags: int = 10, min_score: float = 0.6,\n",
        "                     use_semantic_clustering: bool = True,\n",
        "                     similarity_threshold: float = 0.70) -> List[Tuple[str, float]]:\n",
        "\n",
        "        phrases = self.extractor.extract_phrases(text)\n",
        "\n",
        "        if not phrases:\n",
        "            return []\n",
        "\n",
        "        word_count = len(text.split())\n",
        "        scored_phrases = self.scorer.calculate_phrase_scores(phrases, word_count)\n",
        "\n",
        "        if use_semantic_clustering:\n",
        "            #use semantic clustering to group similar phrases\n",
        "            filtered_phrases = self.scorer.semantic_clustering(\n",
        "                scored_phrases,\n",
        "                self.encoder,\n",
        "                similarity_threshold\n",
        "            )\n",
        "        else:\n",
        "            filtered_phrases = self.scorer.filter_similar_phrases(scored_phrases)\n",
        "\n",
        "        #apply semantic relevance using embeddings\n",
        "        text_embedding = self.encoder.encode_text(text)\n",
        "\n",
        "        #combine quality score with semantic relevance\n",
        "        final_scores = []\n",
        "        for phrase, quality_score in filtered_phrases:\n",
        "            #get semantic similarity between phrase and full text\n",
        "            phrase_embedding = self.encoder.encode_text(phrase)\n",
        "\n",
        "            #calculate cosine similarity\n",
        "            semantic_score = cosine_similarity(\n",
        "                text_embedding.reshape(1, -1),\n",
        "                phrase_embedding.reshape(1, -1)\n",
        "            )[0][0]\n",
        "\n",
        "            #combine scores (70% quality, 30% semantic)\n",
        "            combined_score = (quality_score * 0.7) + (semantic_score * 0.3)\n",
        "\n",
        "            final_scores.append((phrase, combined_score))\n",
        "\n",
        "        final_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        quality_tags = [(tag, score) for tag, score in final_scores if score > min_score]\n",
        "\n",
        "        if len(quality_tags) < 5 and len(final_scores) >= 5:\n",
        "            quality_tags = final_scores[:5]\n",
        "\n",
        "        return quality_tags[:max_tags]\n",
        "\n",
        "    #this returns only tags\n",
        "    def tag_text(self, text: str, max_tags: int = 7) -> List[str]:\n",
        "        tag_scores = self.generate_tags(text, max_tags)\n",
        "        return [tag for tag, _ in tag_scores]\n",
        "\n",
        "    #this returns tags with scores\n",
        "    def tag_text_with_scores(self, text: str, max_tags: int = 7) -> List[Tuple[str, float]]:\n",
        "        return self.generate_tags(text, max_tags)\n",
        "\n",
        "    #tags, score, source\n",
        "    def generate_tags_with_source(self, text: str, max_tags: int = 10, min_score: float = 0.3,\n",
        "                            use_semantic_clustering: bool = True,\n",
        "                            similarity_threshold: float = 0.75) -> List[Tuple[str, float, str]]:\n",
        "        #extract phrases this time along with NER\n",
        "        phrases_with_metadata = self.extractor.extract_phrases_with_metadata(text)\n",
        "\n",
        "        if not phrases_with_metadata:\n",
        "            return []\n",
        "\n",
        "        #score including ner\n",
        "        word_count = len(text.split())\n",
        "        scored_phrases = self.scorer.calculate_phrase_scores_with_ner(phrases_with_metadata, word_count)\n",
        "\n",
        "\n",
        "        if use_semantic_clustering:\n",
        "          # First extract just phrases and scores for clustering\n",
        "          phrases_scores = [(phrase, score) for phrase, score, _ in scored_phrases]\n",
        "\n",
        "          # Apply semantic clustering\n",
        "          clustered = self.scorer.semantic_clustering(\n",
        "              phrases_scores,\n",
        "              self.encoder,\n",
        "              similarity_threshold\n",
        "          )\n",
        "\n",
        "          # Rebuild with metadata - find original metadata for each clustered phrase\n",
        "          filtered_phrases = []\n",
        "          for phrase, score in clustered:\n",
        "              # Find the original metadata\n",
        "              for orig_phrase, orig_score, metadata in scored_phrases:\n",
        "                  if orig_phrase == phrase:\n",
        "                      filtered_phrases.append((phrase, score, metadata))\n",
        "                      break\n",
        "        else:\n",
        "             filtered_phrases = self.scorer.filter_similar_phrases_with_metadata(scored_phrases)\n",
        "\n",
        "        #applying semantic embeddings for clustering similar phrases\n",
        "        text_embedding = self.encoder.encode_text(text)\n",
        "\n",
        "        #combine scores\n",
        "        final_scores = []\n",
        "        for phrase, quality_score, metadata in filtered_phrases:\n",
        "            #get semantic similairty of code and full text\n",
        "            phrase_embedding = self.encoder.encode_text(phrase)\n",
        "\n",
        "            #calculate cosine similarity\n",
        "            semantic_score = cosine_similarity(\n",
        "                text_embedding.reshape(1, -1),\n",
        "                phrase_embedding.reshape(1, -1)\n",
        "            )[0][0]\n",
        "\n",
        "            #combine scores\n",
        "            combined_score = (quality_score * 0.85) + (semantic_score * 0.15)\n",
        "\n",
        "            #keep source\n",
        "            source = metadata.get('source', 'pos')\n",
        "            final_scores.append((phrase, combined_score, source))\n",
        "\n",
        "\n",
        "        final_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        #normalize scores\n",
        "        if final_scores:\n",
        "            max_score = final_scores[0][1]\n",
        "            if max_score > 0:\n",
        "                final_scores = [(tag, score / max_score, source) for tag, score, source in final_scores]\n",
        "\n",
        "        quality_tags = [(tag, score, source) for tag, score, source in final_scores if score > min_score]\n",
        "\n",
        "        if len(quality_tags) < 5 and len(final_scores) >= 5:\n",
        "            quality_tags = final_scores[:5]\n",
        "\n",
        "        return quality_tags[:max_tags]"
      ],
      "metadata": {
        "id": "ShAawqeWjBq3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**"
      ],
      "metadata": {
        "id": "KiuBzvhkdGVU"
      }
    }
  ]
}