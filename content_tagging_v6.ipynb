{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPd5hFIZ8s6Z8ElMNNMJPC+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ac-26/Automated-Content-Tagging-Provision/blob/main/content_tagging_v6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "s3Zoho773RWc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
        "import torch\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import Counter\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder:\n",
        "  #initialization function\n",
        "  def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "      print(f\"Initializing TextEncoder with model: {model_name}\", flush=True)\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      print(\"Tokenizer loaded\", flush=True)\n",
        "      self.model = AutoModel.from_pretrained(model_name)\n",
        "      print(\"Model loaded\", flush=True)\n",
        "      self.model.eval()\n",
        "      print(\"Model Loaded Successfully\", flush=True)\n",
        "\n",
        "  #encodes text\n",
        "  def encode_text(self, text: str) -> np.ndarray:\n",
        "    inputs = self.tokenizer(text, return_tensors=\"pt\",truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "      outputs = self.model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings.numpy().flatten()"
      ],
      "metadata": {
        "id": "3XdI1bkm3c7_"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **This was my initial approach when I was brainstorming to build this system, this is not being used currently in our system, IT IS ONLY KEPT FOR DEMONSTRATION OF PROJECT EVOLUTION.**"
      ],
      "metadata": {
        "id": "DoUBN2OZZb1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class TagVocabulary:\n",
        "#   #initialization function\n",
        "#   def __init__(self):\n",
        "#     self.tags = [\n",
        "#             # Content Creation\n",
        "#             \"Content Writing\", \"Copywriting\", \"Blog Writing\", \"Article Writing\",\n",
        "#             \"Creative Writing\", \"Technical Writing\", \"Content Strategy\",\n",
        "\n",
        "#             # Marketing\n",
        "#             \"Social Media Marketing\", \"Digital Marketing\", \"Email Marketing\",\n",
        "#             \"Marketing Strategy\", \"Brand Marketing\", \"Influencer Marketing\",\n",
        "\n",
        "#             # Social Media\n",
        "#             \"Social Media\", \"Facebook Marketing\", \"Instagram Marketing\",\n",
        "#             \"Twitter Marketing\", \"LinkedIn Marketing\", \"TikTok Marketing\",\n",
        "\n",
        "#             # Analytics & Testing\n",
        "#             \"A/B Testing\", \"Analytics\", \"Performance Tracking\", \"Data Analysis\",\n",
        "#             \"Audience Research\", \"Market Research\",\n",
        "\n",
        "#             # Advertising\n",
        "#             \"Online Advertising\", \"Social Media Ads\", \"Google Ads\",\n",
        "#             \"Facebook Ads\", \"Digital Advertising\",\n",
        "\n",
        "#             # Skills & Techniques\n",
        "#             \"Communication Skills\", \"Writing Skills\", \"Creative Skills\",\n",
        "#             \"Marketing Skills\", \"Design Skills\",\n",
        "\n",
        "#             # Strategy & Planning\n",
        "#             \"Content Planning\", \"Marketing Planning\", \"Campaign Strategy\",\n",
        "#             \"Audience Targeting\", \"Customer Engagement\"\n",
        "#         ]\n",
        "\n",
        "#     print(f\"Tag vocabulary initialized with {len(self.tags)} tags\")\n",
        "\n",
        "#     #this will return list of tags in our vocabulary\n",
        "#     def get_tags(self) -> List[str]:\n",
        "#         return self.tags.copy()\n",
        "\n",
        "#     #this will add a new tag in our vocabulary\n",
        "#     def add_tag(self, new_tag: str):\n",
        "#         if new_tag not in self.tags:\n",
        "#             self.tags.append(new_tag)\n",
        "#             print(f\"Added new tag: {new_tag}\")\n",
        "#         else:\n",
        "#             print(f\"Tag '{new_tag}' already exists\")"
      ],
      "metadata": {
        "id": "Ulg6UPSU6Pso"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class BasicTagger:\n",
        "#   #initializer function\n",
        "#     def __init__(self):\n",
        "#       self.encoder = TextEncoder()\n",
        "#       self.vocabulary = TagVocabulary()\n",
        "\n",
        "#       self.tag_embeddings = self._encode_all_tags()\n",
        "\n",
        "#     #function to encode all tags before hand\n",
        "#     def _encode_all_tags(self) -> Dict[str, np.ndarray]:\n",
        "#         tag_embeddings = {}\n",
        "#         for tag in self.vocabulary.tags:\n",
        "#             embedding = self.encoder.encode_text(tag)\n",
        "#             tag_embeddings[tag] = embedding\n",
        "\n",
        "#         return tag_embeddings\n",
        "\n",
        "#     #finds tags from our vocaublary that are applicable according to our text input\n",
        "#     def find_matching_tags(self, input_text: str, top_k: int = 10) -> List[Tuple[str, float]]:\n",
        "#         # Encode the input text\n",
        "#         input_embedding = self.encoder.encode_text(input_text)\n",
        "\n",
        "#         similarities = []\n",
        "\n",
        "#         for tag_name, tag_embedding in self.tag_embeddings.items():\n",
        "#             # Calculate cosine similarity\n",
        "#             similarity = cosine_similarity(\n",
        "#                 input_embedding.reshape(1, -1),\n",
        "#                 tag_embedding.reshape(1, -1)\n",
        "#             )[0][0]\n",
        "\n",
        "#             similarities.append((tag_name, float(similarity)))\n",
        "\n",
        "#         similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "#         return similarities[:top_k]"
      ],
      "metadata": {
        "id": "vcARCTEQ6ZOb"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Test our basic tagger\n",
        "# def test_basic_tagger():\n",
        "#     tagger = BasicTagger()\n",
        "\n",
        "#     test_text = \"\"\"\n",
        "#     Creating social media posts is a great way to hone your content writing skills.\n",
        "#     Since posts are typically very short, snappy, and quick, you can easily try out\n",
        "#     different styles of writing and see what people respond to. It's easy to change\n",
        "#     direction and adapt if you need to tweak your writing style since social media\n",
        "#     posts are typically fluid and changeable by nature. You can also practice A/B\n",
        "#     testing with your social media adsâ€”try writing two different posts and sending\n",
        "#     it to similar demographics and see which one performs better.\n",
        "#     \"\"\"\n",
        "\n",
        "#     print(\"Input text:\")\n",
        "#     print(test_text)\n",
        "\n",
        "#     # Find matching tags\n",
        "#     matching_tags = tagger.find_matching_tags(test_text, top_k=15)\n",
        "\n",
        "#     print(\"Top matching tags:\")\n",
        "#     for i, (tag, score) in enumerate(matching_tags, 1):\n",
        "#         print(f\"{i:2d}. {tag:<25} (Score: {score:.3f})\")"
      ],
      "metadata": {
        "id": "5O6vyybt_o1o"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#   test_basic_tagger()"
      ],
      "metadata": {
        "id": "r2BVRTDjbJGX"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dynamic Tag Generation**"
      ],
      "metadata": {
        "id": "KlTi8gxagMGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **This is the approach that is being followed right now, it is able to fix the problems and limitation that we were facing in the above approach**"
      ],
      "metadata": {
        "id": "qzTuHgYDcklF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this extracts key phrases from text using linguistic patterns and statistical methods.\n",
        "class KeyPhraseExtractor:\n",
        "    def __init__(self):\n",
        "        # trying to use spacy model for linguistic analysis\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except:\n",
        "            print(\"Please install spacy model: python -m spacy download en_core_web_sm\")\n",
        "            raise\n",
        "\n",
        "        #I have used Parts of Speech(POS) concept for tag extraction\n",
        "        self.phrase_patterns = [\n",
        "            #1-word phrases\n",
        "            [\"NOUN\"],\n",
        "            [\"PROPN\"],\n",
        "\n",
        "            # 2-word phrases\n",
        "            [\"ADJ\", \"NOUN\"],\n",
        "            [\"ADJ\", \"PROPN\"],\n",
        "            [\"NOUN\", \"NOUN\"],\n",
        "            [\"PROPN\", \"NOUN\"],\n",
        "            [\"NOUN\", \"PROPN\"],\n",
        "\n",
        "            # 3-word phrases\n",
        "            [\"ADJ\", \"NOUN\", \"NOUN\"],\n",
        "            [\"NOUN\", \"NOUN\", \"NOUN\"],\n",
        "            [\"PROPN\", \"PROPN\", \"PROPN\"],\n",
        "            [\"ADJ\", \"ADJ\", \"NOUN\"],\n",
        "            # [\"NOUN\", \"VERB\", \"NOUN\"],\n",
        "\n",
        "            # 4-word phrases\n",
        "            [\"ADJ\", \"NOUN\", \"NOUN\", \"NOUN\"],\n",
        "            [\"NOUN\", \"NOUN\", \"NOUN\", \"NOUN\"],\n",
        "            [\"ADJ\", \"ADJ\", \"NOUN\", \"NOUN\"],\n",
        "            # [\"NOUN\", \"NOUN\", \"VERB\", \"NOUN\"],\n",
        "            # [\"ADJ\", \"NOUN\", \"VERB\", \"NOUN\"],\n",
        "            # [\"NOUN\", \"NOUN\", \"NOUN\", \"VERB\"],\n",
        "\n",
        "            # Verb forms (gerunds)\n",
        "            # [\"VERB\"],  # Will filter for -ing forms\n",
        "            # [\"ADJ\", \"VERB\"],\n",
        "        ]\n",
        "\n",
        "        # Common compound terms that should stay together\n",
        "        self.compound_terms = {\n",
        "            \"machine learning\", \"deep learning\", \"natural language processing\",\n",
        "            \"neural network\", \"data science\", \"artificial intelligence\",\n",
        "            \"computer vision\", \"big data\", \"real time\", \"decision making\",\n",
        "            \"supply chain\", \"customer relationship\", \"human resources\",\n",
        "            \"business process\", \"electronic health\", \"patient care\"\n",
        "        }\n",
        "\n",
        "        print(\"KeyPhraseExtractor initialized successfully\")\n",
        "\n",
        "\n",
        "    #applying Named Entity Recognition(NER) for extracting entities like ORG, PERSON, GPE, DATE, etc.\n",
        "    def extract_named_entities(self, text: str) -> List[Tuple[str, str, int]]:\n",
        "      \"\"\"\n",
        "      Extract named entities from text using spaCy's NER.\n",
        "      Returns: List of (entity_text, entity_label, frequency)\n",
        "      \"\"\"\n",
        "      doc = self.nlp(text)  # Using SpaCy, not BERT\n",
        "\n",
        "      # Count frequency of each entity (case-insensitive)\n",
        "      entity_counter = Counter()\n",
        "      entity_labels = {}  # Store the label for each entity\n",
        "\n",
        "      for ent in doc.ents:\n",
        "          # Normalize the entity text (lowercase for counting)\n",
        "          entity_norm = ent.text.lower().strip()\n",
        "\n",
        "          # Skip very short entities (except important ones like \"UN\", \"AI\")\n",
        "          if len(ent.text) < 3 and ent.label_ not in [\"ORG\", \"GPE\", \"PERSON\"]:\n",
        "              continue\n",
        "\n",
        "          entity_counter[entity_norm] += 1\n",
        "          # Store the original casing and label\n",
        "          if entity_norm not in entity_labels:\n",
        "              entity_labels[entity_norm] = (ent.text, ent.label_)\n",
        "\n",
        "      # Format output: (original_text, label, frequency)\n",
        "      entities = []\n",
        "      for norm_text, freq in entity_counter.items():\n",
        "          original_text, label = entity_labels[norm_text]\n",
        "          entities.append((original_text, label, freq))\n",
        "\n",
        "      return entities\n",
        "\n",
        "\n",
        "    def extract_noun_chunks(self, text: str) -> List[Tuple[str, int]]:\n",
        "        \"\"\"\n",
        "        Extract noun chunks from text using spaCy.\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        chunk_counter = Counter()\n",
        "\n",
        "        for chunk in doc.noun_chunks:\n",
        "            chunk_text = chunk.text.strip()\n",
        "\n",
        "            # Only keep multi-word chunks (2+ words)\n",
        "            if len(chunk_text.split()) >= 2:\n",
        "                chunk_lower = chunk_text.lower()\n",
        "                chunk_counter[chunk_lower] += 1\n",
        "\n",
        "        return list(chunk_counter.items())\n",
        "\n",
        "\n",
        "    #updating extract phrases function to combine POS patterns from before and NER entitites as well\n",
        "    def extract_phrases_with_metadata(self, text: str, min_freq: int = 1) -> List[Tuple[str, int, dict]]:\n",
        "      doc = self.nlp(text.lower())\n",
        "\n",
        "      phrase_data = {}  # one phrase contains -> {'freq': count, 'source': type, 'entity_type': label}\n",
        "\n",
        "      #getting all NER entities from the tetx.\n",
        "      # Get NER entities using BERT\n",
        "      ner_entities = self.extract_named_entities(text)\n",
        "      for entity_text, entity_label, freq in ner_entities:\n",
        "          phrase_lower = entity_text.lower()\n",
        "\n",
        "          if phrase_lower not in phrase_data:\n",
        "              phrase_data[phrase_lower] = {\n",
        "                  'freq': freq,\n",
        "                  'source': 'ner',\n",
        "                  'entity_type': entity_label,\n",
        "                  'original_text': entity_text\n",
        "          }\n",
        "          else:\n",
        "              #if already exists (maybe from POS) update only if NER has higher frequency count\n",
        "              if freq > phrase_data[phrase_lower]['freq']:\n",
        "                  phrase_data[phrase_lower]['freq'] = freq\n",
        "                  phrase_data[phrase_lower]['entity_type'] = entity_label\n",
        "\n",
        "      # After NER extraction, add:\n",
        "      noun_chunks = self.extract_noun_chunks(text)\n",
        "      for chunk_text, freq in noun_chunks:\n",
        "          if chunk_text not in phrase_data:\n",
        "              phrase_data[chunk_text] = {\n",
        "                  'freq': freq,\n",
        "                  'source': 'noun_chunk',\n",
        "                  'entity_type': None,\n",
        "                  'original_text': chunk_text\n",
        "              }\n",
        "\n",
        "      #extract phrases using existing POS patterns\n",
        "      text_lower = text.lower()\n",
        "      for compound in self.compound_terms:\n",
        "          count = text_lower.count(compound)\n",
        "          if count > 0:\n",
        "              if compound not in phrase_data:\n",
        "                  phrase_data[compound] = {\n",
        "                      'freq': count,\n",
        "                      'source': 'pos',\n",
        "                      'entity_type': None,\n",
        "                      'original_text': compound\n",
        "                  }\n",
        "              else:\n",
        "                  #update frequency count if POS found more count\n",
        "                  phrase_data[compound]['freq'] = max(phrase_data[compound]['freq'], count)\n",
        "\n",
        "      #extract POS patterns\n",
        "      for sentence in doc.sents:\n",
        "          tokens = list(sentence)\n",
        "\n",
        "          for start_idx in range(len(tokens)):\n",
        "              for pattern in self.phrase_patterns:\n",
        "                  end_idx = start_idx + len(pattern)\n",
        "                  if end_idx <= len(tokens):\n",
        "                      span_tokens = tokens[start_idx:end_idx]\n",
        "                      pos_sequence = [token.pos_ for token in span_tokens]\n",
        "\n",
        "                      if pos_sequence == pattern:\n",
        "                          phrase_tokens = []\n",
        "                          valid = True\n",
        "\n",
        "                          for i, token in enumerate(span_tokens):\n",
        "                              if len(pattern) == 1 and token.is_stop:\n",
        "                                  valid = False\n",
        "                                  break\n",
        "\n",
        "                              if token.pos_ == \"VERB\" and len(pattern) == 1:\n",
        "                                  if not token.text.endswith(\"ing\"):\n",
        "                                      valid = False\n",
        "                                      break\n",
        "\n",
        "                              if len(pattern) == 1 and len(token.text) < 3:\n",
        "                                  valid = False\n",
        "                                  break\n",
        "\n",
        "                              phrase_tokens.append(token.text)\n",
        "\n",
        "                          if valid and phrase_tokens:\n",
        "                              phrase = \" \".join(phrase_tokens)\n",
        "                              phrase = re.sub(r'\\s+', ' ', phrase).strip()\n",
        "\n",
        "                              #check if it's a subset of existing phrase\n",
        "                              is_subset = False\n",
        "                              for existing in phrase_data:\n",
        "                                  if phrase in existing and phrase != existing:\n",
        "                                      is_subset = True\n",
        "                                      break\n",
        "\n",
        "                              if not is_subset and phrase:\n",
        "                                  if phrase not in phrase_data:\n",
        "                                      phrase_data[phrase] = {\n",
        "                                          'freq': 1,\n",
        "                                          'source': 'pos',\n",
        "                                          'entity_type': None,\n",
        "                                          'original_text': phrase\n",
        "                                      }\n",
        "                                  else:\n",
        "                                      phrase_data[phrase]['freq'] += 1\n",
        "\n",
        "      # convert to output format\n",
        "      phrases_with_metadata = []\n",
        "      for phrase, data in phrase_data.items():\n",
        "        if data['freq'] >= min_freq:\n",
        "            metadata = {\n",
        "                'source': data['source'],\n",
        "                'entity_type': data['entity_type']\n",
        "            }\n",
        "            display_text = data.get('original_text', phrase)\n",
        "            phrases_with_metadata.append((display_text, data['freq'], metadata))\n",
        "\n",
        "      phrases_with_metadata.sort(key=lambda x: (x[1], len(x[0].split())), reverse=True)\n",
        "\n",
        "      return phrases_with_metadata"
      ],
      "metadata": {
        "id": "3F2iiLgcAIZH"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is used to score adn filter the phrases that we extracted above to identify best tags in our input sentance\n",
        "class PhraseScorer:\n",
        "    def __init__(self):\n",
        "        # Common/generic words that make poor tags\n",
        "        self.generic_words = {\n",
        "            'way', 'ways', 'thing', 'things', 'people', 'person', 'time', 'times',\n",
        "            'place', 'places', 'day', 'days', 'year', 'years', 'good', 'bad',\n",
        "            'great', 'nice', 'sure', 'certain', 'different', 'same', 'other',\n",
        "            'new', 'old', 'high', 'low', 'large', 'small', 'long', 'short',\n",
        "            'easy', 'hard', 'simple', 'complex', 'nature', 'type', 'types',\n",
        "            'kind', 'kinds', 'lot', 'lots', 'direction', 'need', 'needs'\n",
        "        }\n",
        "\n",
        "        # Words that boost phrase importance\n",
        "        self.domain_indicators = {\n",
        "            'analysis', 'strategy', 'marketing', 'development', 'management',\n",
        "            'design', 'research', 'optimization', 'system', 'process', 'method',\n",
        "            'technique', 'approach', 'framework', 'model', 'algorithm', 'data',\n",
        "            'content', 'digital', 'social', 'media', 'online', 'software',\n",
        "            'testing', 'planning', 'writing', 'creative', 'technical',\n",
        "            'learning', 'training', 'network', 'neural', 'artificial',\n",
        "            'intelligence', 'language', 'processing', 'natural', 'automated'\n",
        "        }\n",
        "\n",
        "        print(\"PhraseScorer initialized successfully\")\n",
        "\n",
        "    #encorporated NER entities to have higher weightage\n",
        "    def calculate_phrase_scores_with_ner(self, phrases_with_metadata: List[Tuple[str, int, dict]],\n",
        "                                    text_length: int) -> List[Tuple[str, float, dict]]:\n",
        "\n",
        "      scored_phrases = []\n",
        "\n",
        "      #get max frequency for normalization\n",
        "      max_freq = 1\n",
        "      if phrases_with_metadata:\n",
        "          frequencies = []\n",
        "          for phrase, freq, metadata in phrases_with_metadata:\n",
        "              frequencies.append(freq)\n",
        "          max_freq = max(frequencies)\n",
        "\n",
        "      for phrase, freq, metadata in phrases_with_metadata:\n",
        "          #initialize scores\n",
        "          scores = {\n",
        "              'frequency': 0.0,\n",
        "              'specificity': 0.0,\n",
        "              'length': 0.0,\n",
        "              'completeness': 0.0,\n",
        "              'entity_bonus': 0.0\n",
        "          }\n",
        "\n",
        "          #frequency score\n",
        "          scores['frequency'] = min(freq / max_freq, 1.0) * 0.3\n",
        "\n",
        "          # secificity score\n",
        "          words = phrase.lower().split()\n",
        "          generic_count = sum(1 for word in words if word in self.generic_words)\n",
        "          scores['specificity'] = (1 - generic_count / len(words)) * 0.25\n",
        "\n",
        "          # 3.length score\n",
        "          if len(words) == 1:\n",
        "              scores['length'] = 0.65\n",
        "          elif len(words) == 2:\n",
        "              scores['length'] = 0.85\n",
        "          elif len(words) == 3:\n",
        "              scores['length'] = 0.90\n",
        "          elif len(words) == 4:\n",
        "              scores['length'] = 1.0\n",
        "          else:\n",
        "              scores['length'] = 0.4\n",
        "          scores['length'] *= 0.15\n",
        "\n",
        "          # completeness score\n",
        "          incomplete_markers = {'of', 'to', 'for', 'with', 'and', 'or', 'but', 'the', 'a', 'an'}\n",
        "          is_complete = (words[0] not in incomplete_markers and\n",
        "                        words[-1] not in incomplete_markers)\n",
        "          scores['completeness'] = 1.0 if is_complete else 0.5\n",
        "          scores['completeness'] *= 0.05\n",
        "\n",
        "          #-----------------------------------------------------------------\n",
        "          #NER scores\n",
        "          if metadata.get('source') == 'ner' and metadata.get('entity_type'):\n",
        "            entity_type = metadata['entity_type']\n",
        "            if entity_type == 'PERSON':\n",
        "                scores['entity_bonus'] = 0.05  # Was 0.20\n",
        "            elif entity_type == 'ORG':\n",
        "                scores['entity_bonus'] = 0.05  # Was 0.20\n",
        "            elif entity_type == 'GPE':\n",
        "                scores['entity_bonus'] = 0.05  # Was 0.25\n",
        "            elif entity_type == 'DATE':\n",
        "                scores['entity_bonus'] = 0.03  # Was 0.15\n",
        "            else:\n",
        "                scores['entity_bonus'] = 0.02  # Was 0.10\n",
        "\n",
        "          # Calculate total score\n",
        "          total_score = sum(scores.values())\n",
        "\n",
        "          scored_phrases.append((phrase, total_score, metadata))\n",
        "\n",
        "      scored_phrases.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "      return scored_phrases\n",
        "\n",
        "\n",
        "    #This removes similar or redundant phrases keeping the ones that are most meaningful\n",
        "    def filter_similar_phrases_with_metadata(self, scored_phrases: List[Tuple[str, float, dict]],\n",
        "                                        similarity_threshold: float = 0.5) -> List[Tuple[str, float, dict]]:\n",
        "\n",
        "      if not scored_phrases:\n",
        "          return []\n",
        "\n",
        "      filtered = []\n",
        "\n",
        "      for phrase, score, metadata in scored_phrases:\n",
        "          phrase_lower = phrase.lower()\n",
        "          words = set(phrase_lower.split())\n",
        "\n",
        "          should_keep = True\n",
        "          phrases_to_remove = []\n",
        "\n",
        "          for i, (kept_phrase, kept_score, kept_metadata) in enumerate(filtered):\n",
        "              kept_lower = kept_phrase.lower()\n",
        "              kept_words = set(kept_lower.split())\n",
        "\n",
        "              # Skip if exact same phrase\n",
        "              if phrase_lower == kept_lower:\n",
        "                  should_keep = False\n",
        "                  break\n",
        "\n",
        "              # Handle subset relationships\n",
        "              if words.issubset(kept_words) or kept_words.issubset(words):\n",
        "                  len_diff = abs(len(words) - len(kept_words))\n",
        "                  score_diff = abs(score - kept_score)\n",
        "\n",
        "                  # Prefer longer phrase if score difference is small\n",
        "                  if score_diff < 0.15 and len_diff >= 1:\n",
        "                      if len(words) > len(kept_words):\n",
        "                          phrases_to_remove.append(i)\n",
        "                      else:\n",
        "                          should_keep = False\n",
        "                          break\n",
        "                  else:\n",
        "                      # Large score difference - keep higher scoring one\n",
        "                      if score > kept_score:\n",
        "                          phrases_to_remove.append(i)\n",
        "                      else:\n",
        "                          should_keep = False\n",
        "                          break\n",
        "\n",
        "          # Remove marked phrases\n",
        "          if phrases_to_remove:\n",
        "              for idx in reversed(phrases_to_remove):\n",
        "                  filtered.pop(idx)\n",
        "\n",
        "          if should_keep:\n",
        "              filtered.append((phrase, score, metadata))\n",
        "\n",
        "      return filtered"
      ],
      "metadata": {
        "id": "037iMS-hi-61"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete dynamic tagging class\n",
        "class DynamicTagger:\n",
        "    def __init__(self, encoder_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        print(\"Initializing DynamicTagger...\", flush=True)\n",
        "        self.encoder = TextEncoder(encoder_model)\n",
        "        print(\"TextEncoder initialized\", flush=True)\n",
        "        self.extractor = KeyPhraseExtractor()\n",
        "        print(\"KeyPhraseExtractor initialized\", flush=True)\n",
        "        self.scorer = PhraseScorer()\n",
        "        print(\"PhraseScorer initialized\", flush=True)\n",
        "        print(\"DynamicTagger ready!\", flush=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #added NER approach\n",
        "    def generate_tags(self, text: str, max_tags: int = 10, min_score: float = 0.15) -> List[Tuple[str, float]]:\n",
        "\n",
        "      # Extract key phrases with metadata (includes NER entities)\n",
        "      phrases_with_metadata = self.extractor.extract_phrases_with_metadata(text)\n",
        "\n",
        "      if not phrases_with_metadata:\n",
        "          return []\n",
        "\n",
        "      # Score phrases including NER bonuses\n",
        "      word_count = len(text.split())\n",
        "      scored_phrases = self.scorer.calculate_phrase_scores_with_ner(phrases_with_metadata, word_count)\n",
        "\n",
        "      # Filter similar phrases (now preserves metadata)\n",
        "      filtered_phrases = self.scorer.filter_similar_phrases_with_metadata(scored_phrases)\n",
        "\n",
        "      # Apply semantic relevance using embeddings\n",
        "      text_embedding = self.encoder.encode_text(text)\n",
        "\n",
        "      # Combine quality score with semantic relevance\n",
        "      final_scores = []\n",
        "      for phrase, quality_score, metadata in filtered_phrases:\n",
        "          # Get semantic similarity between phrase and full text\n",
        "          phrase_embedding = self.encoder.encode_text(phrase)\n",
        "\n",
        "          # Calculate cosine similarity\n",
        "          semantic_score = cosine_similarity(\n",
        "              text_embedding.reshape(1, -1),\n",
        "              phrase_embedding.reshape(1, -1)\n",
        "          )[0][0]\n",
        "\n",
        "          # Combine scores (70% quality, 30% semantic)\n",
        "          combined_score = (quality_score * 0.7) + (semantic_score * 0.3)\n",
        "\n",
        "          final_scores.append((phrase, combined_score))\n",
        "\n",
        "      # Sort by combined score\n",
        "      final_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "      # Apply quality threshold\n",
        "      quality_tags = [(tag, score) for tag, score in final_scores if score > min_score]\n",
        "\n",
        "      # Ensure minimum number of tags\n",
        "      if len(quality_tags) < 5 and len(final_scores) >= 5:\n",
        "          quality_tags = final_scores[:5]\n",
        "\n",
        "      # Return up to max_tags\n",
        "      return quality_tags[:max_tags]\n",
        "\n",
        "\n",
        "    #this returns only tags\n",
        "    def tag_text(self, text: str, max_tags: int = 7) -> List[str]:\n",
        "        tag_scores = self.generate_tags(text, max_tags)\n",
        "        return [tag for tag, _ in tag_scores]\n",
        "\n",
        "    #this returns tags with scores\n",
        "    def tag_text_with_scores(self, text: str, max_tags: int = 7) -> List[Tuple[str, float]]:\n",
        "        return self.generate_tags(text, max_tags)\n"
      ],
      "metadata": {
        "id": "ShAawqeWjBq3"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**"
      ],
      "metadata": {
        "id": "KiuBzvhkdGVU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BB5Ud9X2tfAC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}